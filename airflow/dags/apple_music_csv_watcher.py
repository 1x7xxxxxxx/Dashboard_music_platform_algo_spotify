"""
DAG Apple Music CSV Watcher - Ingestion et Historisation
FrÃ©quence : Toutes les 15 minutes
Description : 
1. DÃ©tecte les CSV dans data/raw/apple_music
2. Met Ã  jour la table 'apple_songs_performance' (Dernier Ã©tat connu)
3. InsÃ¨re une ligne dans 'apple_songs_history' (Snapshot quotidien pour calculs)
4. Archive le fichier traitÃ©
"""

from airflow import DAG
from airflow.operators.python import PythonOperator, BranchPythonOperator
from airflow.operators.empty import EmptyOperator
from datetime import datetime, timedelta, date
import sys
import os
import logging
from pathlib import Path

# Ajouter le chemin pour trouver les modules src
sys.path.insert(0, '/opt/airflow')

logger = logging.getLogger(__name__)

default_args = {
    'owner': 'data_team',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

def check_for_new_csv(**context):
    """VÃ©rifie s'il y a de nouveaux CSV Ã  traiter."""
    try:
        raw_dir = Path('/opt/airflow/data/raw/apple_music')
        raw_dir.mkdir(parents=True, exist_ok=True)
        
        csv_files = list(raw_dir.glob('*.csv'))
        
        logger.info(f'ğŸ“ Scan du dossier: {raw_dir}')
        logger.info(f'ğŸ“Š {len(csv_files)} fichier(s) CSV trouvÃ©(s)')
        
        if csv_files:
            # On transmet la liste des fichiers sous forme de chaÃ®nes de caractÃ¨res
            file_paths = [str(f) for f in csv_files]
            context['task_instance'].xcom_push(key='csv_files', value=file_paths)
            return 'process_csv_files'
        else:
            return 'skip_processing'
        
    except Exception as e:
        logger.error(f'âŒ Erreur scan dossier: {e}')
        return 'skip_processing'


def process_csv_files(**context):
    """Traite les CSV : Upsert Performance + Insert History."""
    try:
        # Imports Ã  l'intÃ©rieur de la fonction pour Ã©viter les erreurs de Top-Level code
        from src.transformers.apple_music_csv_parser import AppleMusicCSVParser
        from src.database.postgres_handler import PostgresHandler
        
        logger.info('='*70)
        logger.info('ğŸ TRAITEMENT CSV APPLE MUSIC')
        logger.info('='*70)
        
        # RÃ©cupÃ©rer la liste des fichiers
        csv_files = context['task_instance'].xcom_pull(
            task_ids='check_new_csv',
            key='csv_files'
        )
        
        if not csv_files:
            return 0
        
        # Initialiser Parser et DB
        parser = AppleMusicCSVParser()
        
        db = PostgresHandler(
            host=os.getenv('DATABASE_HOST', 'postgres'),
            port=int(os.getenv('DATABASE_PORT', 5432)),
            database=os.getenv('DATABASE_NAME', 'spotify_etl'),
            user=os.getenv('DATABASE_USER', 'postgres'),
            password=os.getenv('DATABASE_PASSWORD')
        )
        
        processed_count = 0
        
        for file_path_str in csv_files:
            csv_file = Path(file_path_str)
            if not csv_file.exists():
                continue
                
            logger.info(f'\nğŸ“„ Traitement : {csv_file.name}')
            
            # 1. Parsing
            result = parser.parse_csv_file(csv_file)
            csv_type = result.get('type')
            data = result.get('data')
            
            if not data:
                logger.warning(f"âš ï¸ Aucune donnÃ©e ou type inconnu pour {csv_file.name}")
                continue
            
            # 2. Ingestion selon le type
            try:
                # On se concentre sur le rapport principal "Songs Performance"
                if csv_type == 'songs_performance':
                    
                    # A. UPSERT : Mettre Ã  jour l'Ã©tat actuel (Performance globale)
                    # Cela sert pour les KPIs "Total Ã  date"
                    count_perf = db.upsert_many(
                        table='apple_songs_performance',
                        data=data,
                        conflict_columns=['song_name'],
                        update_columns=[
                            'album_name', 'plays', 'listeners',
                            'shazam_count', 'radio_spins', 'purchases',
                            'collected_at'
                        ]
                    )
                    logger.info(f"   âœ… Performance mise Ã  jour : {count_perf} lignes")

                    # B. INSERT : CrÃ©er un snapshot pour l'historique (Calculs quotidiens)
                    # On extrait juste ce qui est nÃ©cessaire pour le calcul J - (J-1)
                    history_data = []
                    current_date = date.today()
                    timestamp = datetime.now()
                    
                    for row in data:
                        history_data.append({
                            'song_name': row['song_name'],
                            'plays': row['plays'],            # Valeur cumulÃ©e Ã  ce jour
                            'shazam_count': row.get('shazam_count', 0),
                            'date': current_date,
                            'collected_at': timestamp
                        })
                    
                    # Nettoyage prÃ©ventif : Si on relance le script 2 fois le mÃªme jour,
                    # on supprime d'abord les entrÃ©es d'aujourd'hui pour Ã©viter les doublons
                    for row in history_data:
                        db.execute_query(
                            "DELETE FROM apple_songs_history WHERE song_name = %s AND date = %s",
                            (row['song_name'], row['date'])
                        )
                    
                    # Insertion propre
                    count_hist = db.insert_many('apple_songs_history', history_data)
                    logger.info(f"   âœ… Historique sauvegardÃ© : {count_hist} lignes (Snapshot du jour)")

                else:
                    # Pour les autres types (si jamais tu en remets), on log juste
                    logger.info(f"   â„¹ï¸ Type '{csv_type}' ignorÃ© (focus sur performance/history)")

                # 3. Archivage
                archive_dir = Path('/opt/airflow/data/processed/apple_music')
                archive_dir.mkdir(parents=True, exist_ok=True)
                
                timestamp_str = datetime.now().strftime('%Y%m%d_%H%M%S')
                new_name = f"{csv_file.stem}_{timestamp_str}{csv_file.suffix}"
                
                csv_file.rename(archive_dir / new_name)
                logger.info(f"   ğŸ“¦ ArchivÃ© sous : {new_name}")
                processed_count += 1
                
            except Exception as e:
                logger.error(f"âŒ Erreur sur le fichier {csv_file.name}: {e}")
                # On continue vers le fichier suivant mÃªme si celui-ci plante
                continue
        
        db.close()
        logger.info(f"ğŸ Fin du traitement. {processed_count} fichiers traitÃ©s.")
        return processed_count

    except Exception as e:
        logger.error(f"âŒ Erreur fatale DAG : {e}")
        raise


with DAG(
    dag_id='apple_music_csv_watcher',
    default_args=default_args,
    description='Ingestion Apple Music (Performance + Snapshot Historique)',
    schedule_interval='*/15 * * * *',  # Toutes les 15 minutes
    start_date=datetime(2025, 1, 20),
    catchup=False,
    tags=['apple', 'csv', 'production'],
    max_active_runs=1
) as dag:

    check_task = BranchPythonOperator(
        task_id='check_new_csv',
        python_callable=check_for_new_csv,
        provide_context=True
    )

    process_task = PythonOperator(
        task_id='process_csv_files',
        python_callable=process_csv_files,
        provide_context=True
    )

    skip_task = EmptyOperator(task_id='skip_processing')
    end_task = EmptyOperator(task_id='end', trigger_rule='none_failed_min_one_success')

    # Orchestration
    check_task >> [process_task, skip_task]
    process_task >> end_task
    skip_task >> end_task